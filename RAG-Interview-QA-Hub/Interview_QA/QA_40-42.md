Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q40: What are the common retrieval approaches used in RAG systems?

### âœ… Answer

Common retrieval approaches in RAG systems include dense retrieval, sparse retrieval, and hybrid retrieval. Dense retrieval uses embeddings to capture semantic similarity, enabling effective query matching to relevant document chunks. Sparse retrieval relies on traditional methods like TF-IDF or BM25, focusing on keyword-based matching for efficiency. 

Hybrid retrieval combines dense and sparse methods to balance semantic understanding and computational speed. These approaches ensure relevant context is retrieved for generating accurate responses in RAG systems.

## ğŸ“Œ Q41: What are some common challenges in RAG retrieval?

### âœ… Answer

Common challenges in RAG retrieval include ineffective query understanding, scalability issues, context fragmentation, and handling multimodal data.  Ineffective query understanding leads to misinterpreting user intent, resulting in irrelevant retrieved document chunks.  

Scalability issues arise when large-scale data retrieval slows performance or overwhelms the system.   Context fragmentation happens when retrieved chunks lack sufficient context, lowering response quality.   Handling multimodal data is challenging due to complexities in integrating text, images, or other formats effectively.

## ğŸ“Œ Q42: What are the key metrics for evaluating retrieval quality in RAG?

### âœ… Answer

Key metrics for evaluating retrieval quality in RAG systems are precision, recall, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). Precision measures the proportion of retrieved document chunks that are relevant, while recall assesses the proportion of relevant document chunks retrieved from the total available. 

MRR evaluates the ranking quality by considering the position of the first relevant document chunks, and NDCG accounts for the relevance and ranking of retrieved documents. These metrics collectively ensure the retriever effectively identifies and ranks relevant information.


## ğŸ‘¨ğŸ»â€ğŸ’» LLM Survey Papers Collection

ğŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------





