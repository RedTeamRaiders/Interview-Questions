Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q37: How does the chunking strategy differ when dealing with structured documents (like PDFs with tables and figures) versus plain text documents?

### âœ… Answer

Chunking strategies for structured documents like PDFs with tables and figures differ significantly from plain text chunking due to the need to preserve complex layouts and relationships.  For structured documents, the chunking strategy must respect document elements such as tables, figures, headers, and pages to maintain context and semantic meaning. 

Agentic and recursive chunking are more suitable for structured documents due to their flexibility in respecting structure and context. Fixed-size and semantic chunking are often better suited for plain text documents where semantic coherence and simplicity are prioritized.

## ğŸ“Œ Q38: What are the possible reasons for the poor performance of a RAG retriever?

### âœ… Answer

The possible reasons for the poor performance of a RAG retriever are an outdated or incomplete knowledge base, a weak retrieval model, low-quality embeddings, and lack of domain-specific fine-tuning.

An outdated or incomplete knowledge base prevents the retriever from accessing recent or relevant information, limiting answer accuracy. A weak retrieval model, such as using TF-IDF or BM25 instead of dense vector models, leads to less effective retrieval of relevant context.

Low-quality embeddings reduce the semantic understanding between queries and document chunks, causing mismatches. Lack of domain-specific fine-tuning results in retrieval errors because the embedding model doesnâ€™t fully capture the nuances or terminology of the target domain.

## ğŸ“Œ Q39: What happens with a weak retriever in Retrieval-Augmented Generation (RAG) systems?

### âœ… Answer

A weak retriever in RAG systems leads to the retrieval of irrelevant or noisy document chunks. This can significantly degrade the quality of generated answers, as the RAG generator relies heavily on the retrieved context. The presence of irrelevant or noisy document chunks in the context because of poor retrieval causes the generator model to produce answers that are inaccurate or hallucinated while still appearing fluent. 

Therefore, strong retrievers are necessary to provide the most relevant context and ensure factual and relevant outputs in RAG systems.

**ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**
--------------------------------------------------------------------------------------------
ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLM, RAG and Agents. 

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)

---------------------------------------------------------------------------------------------





