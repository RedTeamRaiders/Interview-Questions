Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q1: Explain why RAG is required when LLMs are already powerful.

### âœ… Answer

LLMs are powerful, as they are trained on large volumes of data using sophisticated techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to answer queries related to the latest events or the data not present in their training corpus. 

RAG addresses this challenge by retrieving relevant context from external knowledge sources, which allows LLMs to provide accurate responses. This is why RAG is essential for LLM-based applications that need to be accurate. Otherwise, LLMs alone might provide you answers that are incomplete or outdated.


## ğŸ“Œ Q2: Is RAG still relevant in the era of long context LLMs?

### âœ… Answer

RAG is still important even with long context LLMs. This is because long-context LLMs without RAG have three big problems: *lost in the middle*, *high API costs*, and *increased latency*.

Long-context LLMs often struggle to find the most relevant  information in large contexts, which hurts the quality of generated responses. Furthermore, processing lengthy sequences in each API call results in high latency and high API costs.

RAG addresses these issues by providing the most relevant information from external knowledge sources. So, you still need RAG to get accurate and cost-efficient responses, even with long context LLMs.


## ğŸ“Œ Q3: What are the fundamental challenges of RAG systems?

### âœ… Answer

RAG is powerful, but it has to deal with the following challenges:

- *Scalability*: Searching and retrieving from large, dynamic knowledge sources quickly and efficiently requires a lot of computing power and well-optimized indexing, which can be expensive or take a long time.

- *Latency* - The two-step process (retrieval then generation) can cause delays, making it less suitable for real-time applications without careful optimization.

- *Hallucination Risk* - Even with retrieval, the model might generate plausible but unsupported details if the retrieved data is ambiguous or insufficient.

- *Bias and Noise* - Retrieved content might carry biases, errors, or irrelevant noise from the web or other sources, which can propagate into the output.



## **ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**

ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents.

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)
 

---------------------------------------------------------------------------------------------




