Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q79: Compare and contrast â€œorder-awareâ€ and â€œorder-unawareâ€ retrieval metrics in RAG, giving examples for each from the set (Precision, Recall, MRR, MAP, NDCG).

### âœ… Answer

Order-aware retrieval metrics consider the ranking of retrieved items, emphasizing the importance of higher-ranked relevant results. For example, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) are order-aware, as MRR evaluates the rank of the first relevant item and NDCG accounts for relevance scores and ranking positions.

Order-unaware metrics focus solely on whether relevant items are retrieved and ignore the order.  Precision and Recall are order-unaware, measuring the proportion of relevant items retrieved (Precision) and the proportion of relevant items found out of all relevant items (Recall), without considering their order. 

## ğŸ“Œ Q80: How would the value of NDCG@k change if all relevant chunks are retrieved but in the reverse order (least to most relevant)?

### âœ… Answer


NDCG@k rewards placing highly relevant chunks at earlier ranks and applies a logarithmic discount to relevance scores at lower positions. So reversing the order pushes the most relevant chunks further down the listâ€”making them less valuable in the NDCG calculation. 
 
While all relevant items are present, their suboptimal positions reduce the overall score since NDCG is sensitive to both the presence and order of relevant items in the top k. The value of NDCG@k will decrease compared to the ideal ranking, but will remain higher than a ranking with irrelevant chunks at the top. 


## ğŸ“Œ Q81: What is the significance of Context Precision@K in evaluating a RAG retriever, and how does it differ from standard Precision@k in traditional information retrieval?

### âœ… Answer

The standard Precision@k in traditional information retrieval just measures the proportion of relevant items among the top-k results and ignores the order. Unlike standard Precision@k,  Context Precision@K not only checks whether relevant chunks are retrieved, but also whether they appear at higher ranks in the context. 

Context Precision@K ensures useful information is prioritized in the retrieved context which greatly impacts the quality of the generated answer.

## **ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**

ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents.

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)
 

---------------------------------------------------------------------------------------------





