Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ðŸ“Œ Q16: What are the key considerations when choosing an LLM for a RAG system?

### âœ… Answer

The key considerations when choosing an LLM for a RAG system are reading-comprehension ability, context window size, and inference speed. Reading-comprehension ability reflects how effectively the model processes the retrieved context to generate accurate responses. 

Context window size is crucial, as longer context models enable RAG systems to effectively include more relevant chunks. However, this must be balanced against cost and latency requirements. 

Additionally, inference speed, infrastructure compatibility, and licensing terms also play a key role in deployment decisions for real-world RAG solutions.

## ðŸ“Œ Q17: How is the prompt provided to the LLM in a RAG system different from a standard, non-RAG prompt?

### âœ… Answer

The prompt provided to the LLM without a RAG setup includes only the user query and the optional instructions. Here, the LLM generates the response based on its knowledge gained during training.

The prompt provided to the LLM with the RAG setup includes the user query, instructions, and relevant context. Here, the LLM generates the response as per the instructions solely based on the provided relevant context. 

## ðŸ“Œ Q18: What are the key hyperparameters in a RAG pipeline?

### âœ… Answer

Chunk size, chunk overlap, embedding dimensionality, retrieval top-k, and retrieval threshold are some of the most important hyperparameters for retrieval in RAG. Temperature and max output length are two important hyperparameters for RAG generation.

The chunk size determines how much text is put into a segment before embedding, influencing the context granularity retrieved. Chunk overlap repeats a set of tokens at chunk boundaries, helping preserve important context across segments. Embedding dimensionality is the vector size used to represent text, which affects retrieval precision and database efficiency.

Retrieval top-k sets the number of most similar chunks returned, directly impacting recall and context diversity in the response. The retrieval threshold is a similarity cutoff that filters retrieved results, ensuring only relevant chunks are selected.

Temperature controls the randomness of generated text, balancing creativity and determinism in model outputs. Max output length limits the number of tokens generated, managing the verbosity and computational cost of responses.

## LLM Survey Papers Collection

ðŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------




