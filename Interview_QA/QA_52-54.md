Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q52: When do you opt for hybrid search instead of semantic search?

### âœ… Answer

Hybrid search is preferred over pure semantic search when there is a need to balance exact keyword matches with semantic understanding, especially in scenarios where users require both precision and contextual relevance. 

It is ideal for domains where queries may include specific terms, codes, or entities that must be matched exactly, while also benefiting from capturing synonyms or related concepts. 


## ğŸ“Œ Q53: How do you balance relevance and diversity when retrieving document chunks for RAG?

### âœ… Answer

The retrieval step in RAG relies on cosine similarity to identify top-k relevant document chunks. However, one downside of this approach is that it can return highly similar document chunks, leading to redundancy. 

Balancing relevance and diversity is crucial in RAG retrieval to include contextually important yet diverse document chunks, preventing redundancy and capturing a broader range of perspectives. This balance helps when dealing with complex questions, as different viewpoints or unique insights can improve the answer's quality while still being accurate.

Techniques like Maximal Marginal Relevance (MMR) help to select document chunks that are both highly relevant to the query and diverse from each other, reducing redundancy.

## ğŸ“Œ Q54: How do sparse embeddings differ from dense embeddings in terms of keyword matching and retrieval interpretability?

### âœ… Answer

Sparse embeddings provide interpretability and excel at exact keyword matching. These embeddings represent text as high-dimensional vectors with many zeros. In this, each dimension corresponds to a specific term or feature, making retrieval results more understandable. 

In contrast, dense embeddings are low-dimensional, continuous vectors with mostly non-zero values learned from neural networks, capturing semantic relationships and context beyond exact matches. This makes dense embeddings less interpretable but more effective for retrieving semantically related content where keywords do not exactly overlap. 

Thus, sparse embeddings are favored for precise keyword-based retrieval and interpretability, while dense embeddings support richer, context-aware retrieval. Hybrid approaches leverage the strengths of both sparse and dense embeddings to enhance retrieval performance.


## ğŸ‘¨ğŸ»â€ğŸ’» LLM Survey Papers Collection

ğŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------





