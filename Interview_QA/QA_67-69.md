Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ðŸ“Œ Q67: Describe a scenario where a BM25 retrieval might return relevant chunks but in poor ranking order. How would a neural re-ranker specifically address this limitation?

### âœ… Answer

A typical scenario where BM25 retrieval yields relevant document chunks but in poor ranking order arises when the query uses synonyms or phrases that vary from those in the documents. This is because BM25â€™s exact keyword matching may surface all relevant items, but fail to prioritize those most contextually aligned due to its lack of semantic understanding. 

For instance, searching for "car maintenance" might retrieve document chunks about "vehicle upkeep" and "automobile servicing," but BM25 may rank less relevant document chunks higher if they have keyword overlaps rather than semantic closeness. Neural re-rankers explicitly address this by leveraging deep contextual and semantic signals, reordering the candidate set to prioritize document chunks that best match the queryâ€™s intent and meaning.

## ðŸ“Œ Q68: If your RAG system serves both simple factual queries and complex analytical questions, how would you decide when to bypass the re-ranker for efficiency while maintaining quality?

### âœ… Answer

To decide when to bypass the re-ranker in a RAG system, queries should be classified based on complexity. Simple factual queries like "What is the capital of France?"  require straightforward and well-known answers. Re-ranker can be skipped for simple factual queries, as the initial retrieval is likely to yield highly relevant results. 

For complex analytical questions, such as those requiring synthesis or reasoning across multiple chunks, the re-ranker should be used to ensure the most relevant chunks are prioritized.

## ðŸ“Œ Q69: Describe the vector pre-computation and storage strategy in a bi-encoder + cross-encoder pipeline. Why can't cross-encoders pre-compute text representations like bi-encoders can?

### âœ… Answer

The RAG pipeline leverages bi-encoders for fast retrieval and cross-encoders for the precise reranking of top candidates.

Bi-encoders pre-compute chunk representations by encoding them into fixed-size dense vectors offline and then storing them in a vector database for efficient retrieval. 

Cross-encoders, however, cannot pre-compute chunk representations because they jointly encode query-chunk pairs, capturing intricate interactions through attention mechanisms, requiring both inputs at inference time to produce a relevance score. 

 
## LLM Survey Papers Collection

ðŸ‘‰ [Repo Link](https://github.com/KalyanKS-NLP/LLM-Survey-Papers-Collection)

![LLM Survey Papers Collection](images/llm-survey-papers-collection.jpg)

---------------------------------------------------------------------------------------------




