Authored by **Kalyan KS**. You can follow him on [Twitter](https://x.com/kalyan_kpl) and [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/) for the latest LLM, RAG and Agent updates.

## ğŸ“Œ Q73: How would you evaluate the effectiveness of a reranker in a RAG system? Which metrics (e.g., MRR, MAP, NDCG) would you prioritize and why?

### âœ… Answer

The effectiveness of a re-ranker in a RAG system is best evaluated using ranking metrics that capture how well it prioritizes relevant chunks. Mean Reciprocal Rank (MRR) is key when the focus is on how quickly the first relevant chunk appears, ideal for question-answering scenarios. 

Mean Average Precision (MAP) is useful when multiple relevant chunks matter, measuring both precision and ranking quality across results. Normalized Discounted Cumulative Gain (NDCG) excels when relevance is graded rather than binary, rewarding the correct order of highly relevant chunks. 

## ğŸ“Œ Q74: Explain the difference between Precision@k and Recall@k in the context of RAG. When might you prefer one over the other?

### âœ… Answer

Precision@k focuses on accuracy of the retrieval by measuring the proportion of the top-k retrieved chunks that are relevant to the query.  Recall@k focuses on completeness of the retrieval by measuring the proportion of all relevant chunks that are retrieved within the top-k results.

You might choose Precision@k when you want to ensure high-quality, relevant chunks to reduce noise. On the other hand, you might choose Recall@k when it is crucial to capture as many relevant chunks as possible.

## ğŸ“Œ Q75: Why is MRR unsuitable when there are multiple relevant chunks per query, and how does MAP address this limitation?

### âœ… Answer

MRR (Mean Reciprocal Rank) considers the rank of the first relevant chunk and disregards the ranks and presence of other relevant chunks. This limitation makes MRR more appropriate for scenarios where a single chunk sufficiently answers the query. 

In contrast, MAP (Mean Average Precision) addresses this by averaging the precision across all relevant ranks, accounting for the presence and order of all relevant chunks. Hence, MAP is preferred over MRR for cases where multiple relevant chunks contribute to answering a query comprehensively.

## **ğŸ‘¨ğŸ»â€ğŸ’» LLM Engineer Toolkit**

ğŸ¤– This repository contains a curated list of 120+ LLM, RAG and Agent related libraries category wise. 

ğŸ‘‰ [Repo link](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) 

This repository is highly useful for Data Scientists, AI/ML Engineers working with LLMs, RAG and Agents. 

![LLM Engineer Toolkit](images/llm-engineer-toolkit.jpg)


---------------------------------------------------------------------------------------------





